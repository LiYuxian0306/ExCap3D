# ExCap3D æ•°æ®åŠ è½½æµç¨‹è¯¦è§£

## ğŸ“Š æ€»ä½“æµç¨‹å›¾

```
main_instance_segmentation.py (Hydra å…¥å£)
    â”‚
    â”œâ”€â†’ get_parameters(cfg)
    â”‚   â””â”€â†’ é…ç½®ç±»åˆ«æ•°ï¼ˆè¯»å– top100.txt å’Œ top100_instance.txtï¼‰
    â”‚
    â””â”€â†’ InstanceSegmentation(cfg) [trainer/trainer.py]
        â”‚
        â”œâ”€â†’ prepare_data() â—„â”€â”€â”€ ã€ç¬¬1é˜¶æ®µï¼šæ•°æ®é›†åˆ›å»ºã€‘
        â”‚   â”‚
        â”‚   â”œâ”€â†’ hydra.utils.instantiate(self.config.data.train_dataset)
        â”‚   â”‚   â””â”€â†’ SemanticSegmentationDataset åˆå§‹åŒ–
        â”‚   â”‚       ï¼ˆæ¥è‡ª scannetpp_simple.yaml çš„ train_dataset é…ç½®ï¼‰
        â”‚   â”‚
        â”‚   â””â”€â†’ hydra.utils.instantiate(self.config.data.validation_dataset)
        â”‚       â””â”€â†’ SemanticSegmentationDataset åˆå§‹åŒ–
        â”‚           ï¼ˆæ¥è‡ª scannetpp_simple.yaml çš„ validation_dataset é…ç½®ï¼‰
        â”‚
        â”œâ”€â†’ train_dataloader() â—„â”€â”€â”€ ã€ç¬¬2é˜¶æ®µï¼šDataLoader åˆ›å»º (è®­ç»ƒæ—¶)ã€‘
        â”‚   â”‚
        â”‚   â”œâ”€â†’ hydra.utils.instantiate(self.config.data.train_collation)
        â”‚   â”‚   â””â”€â†’ VoxelizeCollate å®ä¾‹åŒ–
        â”‚   â”‚       ï¼ˆæ¥è‡ª voxelize_collate.yaml çš„ train_collationï¼‰
        â”‚   â”‚
        â”‚   â””â”€â†’ hydra.utils.instantiate(self.config.data.train_dataloader)
        â”‚       â””â”€â†’ torch.utils.data.DataLoader åˆ›å»º
        â”‚           å‚æ•°ï¼š
        â”‚           - dataset = self.train_dataset (SemanticSegmentationDataset)
        â”‚           - collate_fn = VoxelizeCollate å®ä¾‹
        â”‚           - batch_size = 1
        â”‚           - shuffle = true
        â”‚           - num_workers = 4
        â”‚           - persistent_workers = true
        â”‚
        â””â”€â†’ training_step(batch, batch_idx) â—„â”€â”€â”€ ã€ç¬¬3é˜¶æ®µï¼šæ•°æ®åŠ è½½ä¸å¤„ç†ã€‘
            â”‚
            â”œâ”€â†’ 1ï¸âƒ£ batch = next(iter(dataloader))
            â”‚   â”‚
            â”‚   â”œâ”€ DataLoader ä» train_dataset ä¸­å– batch_size=1 ä¸ªæ ·æœ¬
            â”‚   â”‚
            â”‚   â”œâ”€â†’ å¯¹æ¯ä¸ªæ ·æœ¬è°ƒç”¨ dataset.__getitem__(idx)
            â”‚   â”‚   è¿”å›ï¼š(coordinates, features, labels, scene_id, 
            â”‚   â”‚            raw_color, raw_normals, raw_coordinates, idx, cap_data)
            â”‚   â”‚   è¿™ä¸€æ­¥å‘ç”Ÿåœ¨ DataLoader worker ä¸­
            â”‚   â”‚
            â”‚   â””â”€â†’ 2ï¸âƒ£ è°ƒç”¨ collate_fn (VoxelizeCollate å®ä¾‹)
            â”‚       â”‚
            â”‚       â””â”€â†’ VoxelizeCollate.__call__(batch)
            â”‚           â”‚
            â”‚           â”œâ”€ è¾“å…¥ï¼šbatch = [sample1, sample2, ...]
            â”‚           â”‚   æ¯ä¸ª sample: (coords, feats, labels, scene_id, ...)
            â”‚           â”‚
            â”‚           â”œâ”€ è°ƒç”¨ voxelize() å‡½æ•°
            â”‚           â”‚   â”œâ”€ ä½“ç´ åŒ–åæ ‡ï¼šcoords / 0.04
            â”‚           â”‚   â”œâ”€ MinkowskiEngine å»é‡ï¼šsparse_quantize()
            â”‚           â”‚   â”œâ”€ ç”Ÿæˆå®ä¾‹æ©ç 
            â”‚           â”‚   â””â”€ ç”Ÿæˆåˆ†æ®µæ©ç 
            â”‚           â”‚
            â”‚           â””â”€ è¾“å‡ºï¼š
            â”‚               {
            â”‚                 'coords': SparseTensor coordinates,
            â”‚                 'feats': SparseTensor features,
            â”‚                 'labels': instance labels,
            â”‚                 'masks': instance masks,
            â”‚                 'point2segment': mapping,
            â”‚                 ...
            â”‚               }
            â”‚
            â”œâ”€ 3ï¸âƒ£ data, target, file_names, cap_gt = batch
            â”‚   ï¼ˆè§£åŒ… collate_fn è¿”å›çš„ batchï¼‰
            â”‚
            â”œâ”€ 4ï¸âƒ£ data = ME.SparseTensor(coordinates, features, device)
            â”‚   ï¼ˆåœ¨ GPU ä¸Šåˆ›å»º SparseTensorï¼‰
            â”‚
            â””â”€ 5ï¸âƒ£ output = self.forward(data, ...)
                ï¼ˆæ¨¡å‹å‰å‘ä¼ æ’­ï¼‰
```

---

## ğŸ”„ è¯¦ç»†æ­¥éª¤è¯´æ˜

### **ç¬¬ 1 é˜¶æ®µï¼šæ•°æ®é›†åˆå§‹åŒ–** 
**ä½ç½®**: [trainer/trainer.py#L2950](trainer/trainer.py#L2950-L3000) - `prepare_data()` æ–¹æ³•

**æ—¶æœº**: åœ¨ PyTorch Lightning è®­ç»ƒå¼€å§‹å‰è‡ªåŠ¨è°ƒç”¨

**é…ç½®æ¥æº**:
- train_dataset: [conf/data/datasets/scannetpp_simple.yaml](conf/data/datasets/scannetpp_simple.yaml) çš„ `train_dataset` éƒ¨åˆ†
- validation_dataset: [conf/data/datasets/scannetpp_simple.yaml](conf/data/datasets/scannetpp_simple.yaml) çš„ `validation_dataset` éƒ¨åˆ†

**ä»£ç **:
```python
def prepare_data(self):
    if self.config.general.train_mode:
        # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
        # config.data.train_dataset æ¥è‡ª scannetpp_simple.yaml
        self.train_dataset = hydra.utils.instantiate(self.config.data.train_dataset)
        # SemanticSegmentationDataset åˆå§‹åŒ–
        # ä¸»è¦å‚æ•°ï¼š
        #   - dataset_name="scannetpp"
        #   - data_dir="/home/kylin/lyx/project_study/ExCap3D/data/processed/"
        #   - list_file="/home/kylin/lyx/project_study/ExCap3D/code/excap3d/train_list.txt"
        #   - clip_points=600000
        #   - mode="train" (å¯ç”¨æ•°æ®å¢å¼º)
        
    # åˆ›å»ºéªŒè¯æ•°æ®é›†
    self.validation_dataset = hydra.utils.instantiate(self.config.data.validation_dataset)
    # ä¸»è¦å‚æ•°ï¼š
    #   - dataset_name="scannetpp"
    #   - list_file="/home/kylin/lyx/project_study/ExCap3D/code/excap3d/val_list.txt"
    #   - clip_points=0 (ä¸è£å‰ª)
    #   - mode="validation" (ç¦ç”¨æ•°æ®å¢å¼º)
```

**æ­¤é˜¶æ®µå‘ç”Ÿçš„æ“ä½œ**:
1. åŠ è½½åœºæ™¯åˆ—è¡¨æ–‡ä»¶ (train_list.txt / val_list.txt)
2. æ„å»ºæ•°æ®æ–‡ä»¶è·¯å¾„ç´¢å¼•
3. åŠ è½½æ ‡ç­¾å®šä¹‰ (label_info)
4. åˆå§‹åŒ–æ•°æ®å¢å¼ºé…ç½® (volumentations + albumentations)

---

### **ç¬¬ 2 é˜¶æ®µï¼šDataLoader åˆ›å»º**
**ä½ç½®**: [trainer/trainer.py#L3046](trainer/trainer.py#L3046-L3070) - `train_dataloader()` å’Œ `val_dataloader()` æ–¹æ³•

**æ—¶æœº**: åœ¨ `trainer.fit(model)` è¢«è°ƒç”¨åï¼ŒPyTorch Lightning è‡ªåŠ¨è°ƒç”¨è¿™äº›æ–¹æ³•

**é…ç½®æ¥æº**:
- train_dataloader: [conf/data/data_loaders/simple_loader.yaml](conf/data/data_loaders/simple_loader.yaml) çš„ `train_dataloader`
- train_collation: [conf/data/collation_functions/voxelize_collate.yaml](conf/data/collation_functions/voxelize_collate.yaml) çš„ `train_collation`
- val_dataloader: [conf/data/data_loaders/simple_loader.yaml](conf/data/data_loaders/simple_loader.yaml) çš„ `validation_dataloader`
- validation_collation: [conf/data/collation_functions/voxelize_collate.yaml](conf/data/collation_functions/voxelize_collate.yaml) çš„ `validation_collation`

**ä»£ç **:
```python
def train_dataloader(self):
    # ç¬¬1æ­¥ï¼šåˆ›å»º collate å‡½æ•°å®ä¾‹
    # config.data.train_collation æ¥è‡ª voxelize_collate.yaml
    c_fn = hydra.utils.instantiate(self.config.data.train_collation)
    # c_fn = VoxelizeCollate å®ä¾‹
    # å‚æ•°ï¼š
    #   - voxel_size=0.04
    #   - ignore_label=-100
    #   - task="instance_segmentation"
    #   - segment_strategy="majority_instance"
    #   - ...
    
    # ç¬¬2æ­¥ï¼šåˆ›å»º DataLoader å®ä¾‹
    # config.data.train_dataloader æ¥è‡ª simple_loader.yaml
    return hydra.utils.instantiate(
        self.config.data.train_dataloader,
        self.train_dataset,           # æ•°æ®é›†å®ä¾‹
        collate_fn=c_fn,              # collate å‡½æ•°
    )
    # è¿”å›ï¼štorch.utils.data.DataLoader(
    #     dataset=self.train_dataset,
    #     shuffle=True,
    #     pin_memory=False,
    #     num_workers=4,
    #     batch_size=1,
    #     persistent_workers=True,
    #     collate_fn=VoxelizeCollate(...)
    # )
```

**ä¸‰ä¸ªç»„ä»¶çš„è§’è‰²**:
1. **scannetpp_simple.yaml**: å®šä¹‰æ•°æ®é›†åˆå§‹åŒ–å‚æ•° (train_dataset, validation_dataset)
2. **simple_loader.yaml**: å®šä¹‰ DataLoader åˆå§‹åŒ–å‚æ•° (batch_size, shuffle, num_workers ç­‰)
3. **voxelize_collate.yaml**: å®šä¹‰ collate å‡½æ•° (å¤„ç† batch çš„å‡½æ•°)

---

### **ç¬¬ 3 é˜¶æ®µï¼šæ•°æ®æ‰¹æ¬¡åŠ è½½ä¸å¤„ç†**
**ä½ç½®**: [trainer/trainer.py#L284](trainer/trainer.py#L284-L500) - `training_step()` æ–¹æ³•

**æ—¶æœº**: åœ¨æ¯ä¸ªè®­ç»ƒ stepï¼ŒPyTorch Lightning è‡ªåŠ¨è°ƒç”¨

**æµç¨‹**:

#### **3a. DataLoader è¿­ä»£ (batch åˆ›å»º)**

```
for batch in train_dataloader:
    â”œâ”€ DataLoader å·¥ä½œæµç¨‹ï¼š
    â”‚
    â”œâ”€ 1ï¸âƒ£ ä¸»è¿›ç¨‹ï¼ˆä¸»çº¿ç¨‹ï¼‰ï¼š
    â”‚   â””â”€ DataLoader.iter() åˆ›å»ºè¿­ä»£å™¨
    â”‚
    â”œâ”€ 2ï¸âƒ£ Worker è¿›ç¨‹ï¼ˆ4ä¸ªï¼Œç”± num_workers=4ï¼‰ï¼š
    â”‚   â”‚   æ¯ä¸ª worker æ‰§è¡Œï¼š
    â”‚   â”‚
    â”‚   â”œâ”€ for idx in batch_indices:  # batch_indices = [i] (batch_size=1)
    â”‚   â”‚   â”‚
    â”‚   â”‚   â””â”€ sample = dataset.__getitem__(idx)
    â”‚   â”‚       â”‚
    â”‚   â”‚       â””â”€â†’ [datasets/semseg.py#L595](datasets/semseg.py#L595) SemanticSegmentationDataset.__getitem__()
    â”‚   â”‚
    â”‚   â””â”€ è¿”å›ï¼š[sample1, sample2, ...]  # é•¿åº¦=batch_size
    â”‚
    â”œâ”€ 3ï¸âƒ£ ä¸»è¿›ç¨‹ï¼š
    â”‚   â””â”€ batch = collate_fn([sample1, sample2, ...])
    â”‚       â”‚
    â”‚       â””â”€â†’ [datasets/utils.py#L10] VoxelizeCollate.__call__()
    â”‚
    â””â”€ 4ï¸âƒ£ è¿”å›ç»™è®­ç»ƒå¾ªç¯ï¼š
        â””â”€ data, target, file_names, cap_gt = batch
```

#### **3b. dataset.__getitem__ è¯¦ç»†è¿‡ç¨‹**

**ä½ç½®**: [datasets/semseg.py#L595](datasets/semseg.py#L595-L1100)

```python
def __getitem__(self, idx: int):
    # â‘  åŠ è½½åŸå§‹ç‚¹äº‘æ•°æ®
    points = np.load(filepath)  # Shape: (N, 12)
    coordinates = points[:, :3]
    color = points[:, 3:6]
    normals = points[:, 6:9]
    segments = points[:, 9]
    labels = points[:, 10:12]   # [semantic_id, instance_id]
    
    # â‘¡ ç‚¹æ•°è£å‰ªï¼ˆä»…è®­ç»ƒæ—¶ï¼Œclip_points=600000ï¼‰
    if len(points) > 600000:
        ndx = np.random.choice(len(points), 600000, replace=False)
        points = points[ndx]
    
    # â‘¢ æ•°æ®å¢å¼ºï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
    if "train" in self.mode:
        # åæ ‡å½’ä¸€åŒ–ã€éšæœºå¹³ç§»ã€éšæœºç¿»è½¬ã€å¼¹æ€§å˜å½¢ã€é¢œè‰²å¢å¼ºç­‰
        ...
    
    # â‘£ ç‰¹å¾ç»„åˆ
    # features = [R, G, B, x_raw, y_raw, z_raw] (6 ç»´)
    features = np.hstack((color, coordinates))
    
    # â‘¤ æ ‡ç­¾é‡æ˜ å°„
    labels[:, 0] = _remap_from_zero(labels[:, 0])
    
    # â‘¥ æ ‡ç­¾å †å 
    labels = np.hstack((labels, segments[..., None]))
    # labels Shape: (N, 3) = [semantic_id, instance_id, segment_id]
    
    return (
        coordinates,       # (N, 3)
        features,          # (N, 6)
        labels,            # (N, 3)
        scene_id,          # str
        raw_color,         # (N, 3)
        raw_normals,       # (N, 3)
        raw_coordinates,   # (N, 3)
        idx,               # int
        cap_data_final     # dict
    )
```

#### **3c. VoxelizeCollate è¯¦ç»†è¿‡ç¨‹**

**ä½ç½®**: [datasets/utils.py#L10](datasets/utils.py#L10-L220)

```python
def __call__(self, batch):
    # batch = [sample1, sample2, ...]
    # æ¯ä¸ª sample æ˜¯ dataset.__getitem__ çš„è¾“å‡º
    
    return voxelize(
        batch,
        ignore_label=-100,
        voxel_size=0.04,
        segment_strategy="majority_instance",
        ...
    )

def voxelize(batch, ...):
    # â‘  ä½“ç´ åŒ–åæ ‡
    for sample in batch:
        coords = np.floor(sample[0] / 0.04)  # åæ ‡é‡åŒ–
        
        # â‘¡ å»é‡ï¼ˆMinkowskiEngineï¼‰
        unique_map, inverse_map = ME.utils.sparse_quantize(coords)
        
        # â‘¢ è·å¾—æœ€ç»ˆä½“ç´ åæ ‡å’Œç‰¹å¾
        voxel_coords = coords[unique_map]      # (~100k, 3)
        voxel_features = features[unique_map]  # (~100k, 6)
        
        coordinates.append(torch.from_numpy(voxel_coords).int())
        features.append(torch.from_numpy(voxel_features).float())
        labels.append(torch.from_numpy(labels[unique_map]).long())
    
    # â‘£ æ‰¹æ¬¡ç»„è£…ï¼ˆå¤šåœºæ™¯åˆå¹¶ï¼‰
    input_dict = {
        "coords": coordinates,      # List of (N_voxels, 3) per scene
        "feats": features,          # List of (N_voxels, 6) per scene
        "labels": labels,           # List of (N_voxels, 3) per scene
    }
    
    # â‘¤ åˆ›å»º SparseTensor æ‰€éœ€çš„ batch åæ ‡
    batch_coords = ME.utils.batched_coordinates(coordinates)
    # batch_coords Shape: (Total_voxels, 4) = [batch_id, x, y, z]
    
    # â‘¥ ç”Ÿæˆå®ä¾‹æ©ç 
    for inst_id in unique_instances:
        mask = (labels[:, 1] == inst_id)  # bool æ©ç 
        masks.append(torch.from_numpy(mask).bool())
    
    # â‘¦ ç”Ÿæˆåˆ†æ®µæ©ç 
    segment_mask = aggregate_to_segments(masks, point2segment)
    
    return {
        "data": {
            "coordinates": batch_coords,    # (Total_voxels, 4)
            "features": cat_features,       # (Total_voxels, 6)
        },
        "target": [
            {
                "labels": labels,               # (N_instances,)
                "masks": masks,                 # (N_instances, N_voxels)
                "segment_mask": segment_mask,   # (N_instances, N_segments)
                "inst_ids": inst_ids,
                "point2segment": point2segment,
            },
            ...  # æ¯ä¸ªåœºæ™¯ä¸€ä¸ª
        ],
        "file_names": [scene_id1, scene_id2, ...],
        "cap_gt": [cap_data1, cap_data2, ...],
    }
```

#### **3d. training_step ä¸­çš„æ•°æ®ä½¿ç”¨**

```python
def training_step(self, batch, batch_idx):
    data, target, file_names, cap_gt = batch
    # data: MinkowskiEngine SparseTensor dict
    # target: å®ä¾‹æ©ç å’Œæ ‡ç­¾
    # file_names: åœºæ™¯ ID
    # cap_gt: å­—å¹•æ•°æ®
    
    # â‘  ç§»åˆ° GPU
    data = ME.SparseTensor(
        coordinates=data.coordinates,   # (Total_voxels, 4)
        features=data.features,         # (Total_voxels, 6)
        device=self.device
    )
    
    # â‘¡ æ¨¡å‹å‰å‘ä¼ æ’­
    output = self.forward(data, point2segment=p2s, ...)
    
    # â‘¢ æŸå¤±è®¡ç®—
    losses, assignment = self.criterion(output, target)
```

---

## ğŸ“ é…ç½®æ–‡ä»¶å¯¹åº”å…³ç³»

| é…ç½®æ–‡ä»¶ | æ¥æº | ç”¨é€” | å…³é”®å‚æ•° | åœ¨ç¬¬å‡ é˜¶æ®µä½¿ç”¨ |
|---------|------|------|---------|-----------------|
| **scannetpp_simple.yaml** | `conf/data/datasets/scannetpp_simple.yaml` | å®šä¹‰ SemanticSegmentationDataset åˆå§‹åŒ–å‚æ•° | `dataset_name`, `data_dir`, `list_file`, `clip_points`, `mode`, `image_augmentations_path`, `volume_augmentations_path` | **ç¬¬1é˜¶æ®µ** - `prepare_data()` |
| **simple_loader.yaml** | `conf/data/data_loaders/simple_loader.yaml` | å®šä¹‰ torch.utils.data.DataLoader åˆå§‹åŒ–å‚æ•° | `batch_size`, `shuffle`, `num_workers`, `pin_memory`, `persistent_workers` | **ç¬¬2é˜¶æ®µ** - `train_dataloader()` |
| **voxelize_collate.yaml** | `conf/data/collation_functions/voxelize_collate.yaml` | å®šä¹‰ VoxelizeCollate åˆå§‹åŒ–å‚æ•°ï¼ˆbatch å¤„ç†å‡½æ•°ï¼‰ | `voxel_size`, `task`, `segment_strategy`, `ignore_label`, `filter_out_classes` | **ç¬¬2é˜¶æ®µ** - `train_dataloader()` å’Œ **ç¬¬3é˜¶æ®µ** - æ¯ä¸ª batch è¿­ä»£æ—¶ |

---

## ğŸ”€ ä½¿ç”¨é¡ºåºæ€»ç»“

```
ã€è®­ç»ƒå¼€å§‹ã€‘
    â†“
ã€ç¬¬1é˜¶æ®µã€‘prepare_data()
    â”œâ”€ scannetpp_simple.yaml (train_dataset éƒ¨åˆ†)
    â”‚   â””â”€â†’ åˆ›å»º self.train_dataset (SemanticSegmentationDataset å®ä¾‹)
    â”‚
    â””â”€ scannetpp_simple.yaml (validation_dataset éƒ¨åˆ†)
        â””â”€â†’ åˆ›å»º self.validation_dataset (SemanticSegmentationDataset å®ä¾‹)
    
    â†“
ã€ç¬¬2é˜¶æ®µã€‘train_dataloader()
    â”œâ”€ voxelize_collate.yaml (train_collation)
    â”‚   â””â”€â†’ åˆ›å»º collate_fn (VoxelizeCollate å®ä¾‹)
    â”‚
    â””â”€ simple_loader.yaml (train_dataloader)
        â””â”€â†’ åˆ›å»º DataLoader(
            dataset=self.train_dataset,
            collate_fn=collate_fn,
            ...
        )
    
    â†“
ã€ç¬¬3é˜¶æ®µã€‘for batch in train_dataloader (æ¯ä¸ªè®­ç»ƒ step)
    â”œâ”€ 1ï¸âƒ£ dataset.__getitem__()  
    â”‚  ï¼ˆæ¥è‡ª scannetpp_simple.yaml çš„ dataset å‚æ•°ï¼‰
    â”‚
    â”œâ”€ 2ï¸âƒ£ collate_fn(batch)
    â”‚  ï¼ˆå³ VoxelizeCollate.__call__()ï¼‰
    â”‚
    â””â”€ 3ï¸âƒ£ training_step(batch, batch_idx)
        â””â”€â†’ æ¨¡å‹è®­ç»ƒ
```

---

## ğŸ¯ å…·ä½“å‚æ•°æµåŠ¨ç¤ºä¾‹

```python
# â‘  scannetpp_simple.yaml å®šä¹‰ï¼š
train_dataset:
  _target_: datasets.semseg.SemanticSegmentationDataset
  dataset_name: "scannetpp"
  data_dir: ${data.data_dir}
  list_file: ${data.train_list_file}
  clip_points: ${data.train_dataset.clip_points}
  mode: "train"

# â‘¡ prepare_data() ä¸­ï¼š
self.train_dataset = SemanticSegmentationDataset(
    dataset_name="scannetpp",
    data_dir="/home/kylin/lyx/project_study/ExCap3D/data/processed/",
    list_file="/home/kylin/lyx/project_study/ExCap3D/code/excap3d/train_list.txt",
    clip_points=600000,
    mode="train"
)

# â‘¢ simple_loader.yaml å®šä¹‰ï¼š
train_dataloader:
  _target_: torch.utils.data.DataLoader
  shuffle: true
  batch_size: ${data.batch_size}  # = 1
  num_workers: 4
  persistent_workers: true

# â‘£ train_dataloader() ä¸­ï¼š
return DataLoader(
    dataset=self.train_dataset,        # â† æ¥è‡ªé˜¶æ®µâ‘ 
    batch_size=1,
    shuffle=True,
    num_workers=4,
    persistent_workers=True,
    collate_fn=VoxelizeCollate(...)    # â† æ¥è‡ª voxelize_collate.yaml
)

# â‘¤ voxelize_collate.yaml å®šä¹‰ï¼š
train_collation:
  _target_: datasets.utils.VoxelizeCollate
  voxel_size: 0.04
  task: "instance_segmentation"
  segment_strategy: "majority_instance"

# â‘¥ train_collation åœ¨ train_dataloader() ä¸­å®ä¾‹åŒ–ï¼š
c_fn = VoxelizeCollate(
    voxel_size=0.04,
    task="instance_segmentation",
    segment_strategy="majority_instance",
    ...
)
```

---

## ğŸ“Œ å…³é”®æ—¶åº

| æ—¶åˆ» | äº‹ä»¶ | ä»£ç ä½ç½® |
|------|------|---------|
| æ¨¡å‹åˆå§‹åŒ– | InstanceSegmentation.__init__() | [trainer/trainer.py#L80](trainer/trainer.py#L80-L300) |
| è®­ç»ƒå¼€å§‹å‰ | `prepare_data()` è°ƒç”¨ | [trainer/trainer.py#L2950](trainer/trainer.py#L2950) |
| è®­ç»ƒå¼€å§‹æ—¶ | `train_dataloader()` è°ƒç”¨ | [trainer/trainer.py#L3046](trainer/trainer.py#L3046) |
| æ¯ä¸ª step | `training_step()` è°ƒç”¨ï¼Œè¿­ä»£ batch | [trainer/trainer.py#L284](trainer/trainer.py#L284) |
| batch æ„é€  | DataLoader è°ƒç”¨ `__getitem__()` + `collate_fn()` | [datasets/semseg.py#L595](datasets/semseg.py#L595) + [datasets/utils.py#L46](datasets/utils.py#L46) |
