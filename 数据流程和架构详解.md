# ExCap3D 数据流程、Label映射、模型架构和评估详解

## 1. 整体数据流程详解

### 1.1 数据流程概览

整个数据流程分为以下几个阶段：

```
ScanNetPP原始数据 
  ↓
prepare_training_data.py (scannetpp)
  ↓
.pth文件 (包含vtx_*或sampled_*数据)
  ↓
sample_pth.py (ExCap3D)
  ↓
.pth文件 (统一为vtx_*格式)
  ↓
scannetpp_pth_preprocessing.py (ExCap3D)
  ↓
.npy文件 + GT文件 + 数据库YAML
  ↓
训练时: SemanticSegmentationDataset + VoxelizeCollate
  ↓
模型输入 (SparseTensor)
```

### 1.2 阶段1: ScanNetPP prepare_training_data.py

**文件位置**: `/Users/liyuxian/Desktop/AI_paper_study/Project_study/scannetpp/semantic/prep/prepare_training_data.py`

**输入数据**:
- `mesh_aligned_0.05.ply`: 原始mesh文件（顶点、颜色、面）
- `segments.json`: 每个顶点的segment ID（在ScanNetPP中，每个顶点对应一个segment）
- `segments_anno.json`: 标注信息，包含每个实例的label和segments列表

**处理流程**:

1. **AddMeshVertices** (`semantic/transforms/mesh.py:247-258`)
   - 从mesh提取顶点坐标和颜色
   - 输出: `vtx_coords` (N, 3), `vtx_colors` (N, 3) [0-1范围]

2. **MapLabelToIndex** (`semantic/transforms/mesh.py:10-96`)
   - **关键映射逻辑**:
     - 读取 `top100.txt` (100个语义类别)
     - 读取 `map_benchmark.csv` (原始标签到标准标签的映射)
     - 创建两层映射:
       - `label_mapping`: 原始标签名 → 标准标签名 (如 'books' → 'book')
       - `mapping`: 标准标签名 → 索引 (0-99)
   - 对每个 `segGroups` 中的实例:
     - 获取原始label (如 'books')
     - 通过 `label_mapping` 转换为标准label (如 'book')
     - 通过 `mapping` 转换为索引 (如 15)
     - 存储: `label_ndx` (0-99 或 -100表示忽略)

3. **GetLabelsOnVertices** (`semantic/transforms/mesh.py:122-244`)
   - **核心逻辑**: 将实例级别的标签映射到每个顶点
   - 输入: `segIndices` (每个顶点的segment ID), `segGroups` (实例标注)
   - 处理:
     - 对于每个实例，找到属于该实例的所有顶点
     - 如果顶点属于多个实例，选择最小的实例
     - 对于实例类别（在 `top100_instance.txt` 中），分配instance label
   - 输出:
     - `vtx_labels`: 语义标签索引 (0-99 或 -100)
     - `vtx_instance_labels`: 实例标签索引 (0-N 或 -100，仅对实例类别)
     - `vtx_instance_anno_id`: 原始标注中的objectId

4. **SamplePointsOnMesh** (`semantic/transforms/mesh.py:260-296`)
   - 在mesh上均匀采样点
   - 使用KDTree找到每个采样点的最近顶点
   - 将顶点的属性（labels等）映射到采样点
   - 输出: `sampled_coords`, `sampled_colors`, `sampled_labels`, `sampled_instance_labels`, `sampled_instance_anno_id`

**输出数据结构** (`.pth`文件):
```python
{
    'scene_id': str,
    'vtx_coords': np.array(N, 3),           # 顶点坐标
    'vtx_colors': np.array(N, 3),           # 顶点颜色 [0-1]
    'vtx_labels': np.array(N,),              # 语义标签 [0-99或-100]
    'vtx_instance_labels': np.array(N,),    # 实例标签 [0-N或-100]
    'vtx_instance_anno_id': np.array(N,),   # 原始objectId
    'sampled_coords': np.array(M, 3),       # 采样点坐标
    'sampled_colors': np.array(M, 3),        # 采样点颜色
    'sampled_labels': np.array(M,),         # 采样点语义标签
    'sampled_instance_labels': np.array(M,), # 采样点实例标签
    'sampled_instance_anno_id': np.array(M,), # 采样点实例ID
}
```

### 1.3 阶段2: ExCap3D sample_pth.py

**文件位置**: `/Users/liyuxian/Desktop/AI_paper_study/Project_study/ExCap3D/ExCap3D/sample_pth.py`

**目的**: 在原始mesh上重新采样，统一数据格式为 `vtx_*`

**处理流程**:

1. 读取 `.pth` 文件（可能包含 `vtx_*` 或 `sampled_*`）
2. 读取原始mesh和segments.json
3. 在mesh上重新采样点（数量 = `sample_factor * base_point_count`）
4. 使用KDTree将标签从原始数据映射到新采样点
5. 添加 `vtx_segment_ids`（从segments.json获取）

**输出数据结构**:
```python
{
    'scene_id': str,
    'vtx_coords': np.array(M, 3),           # 新采样点坐标
    'vtx_colors': np.array(M, 3),           # 新采样点颜色 [0-1]
    'vtx_normals': np.array(M, 3),          # 法向量
    'vtx_labels': np.array(M,),            # 语义标签
    'vtx_instance_labels': np.array(M,),   # 实例标签
    'vtx_instance_anno_id': np.array(M,),  # 实例ID
    'vtx_segment_ids': np.array(M,),       # segment ID
}
```

### 1.4 阶段3: ExCap3D scannetpp_pth_preprocessing.py

**文件位置**: `/Users/liyuxian/Desktop/AI_paper_study/Project_study/ExCap3D/ExCap3D/datasets/preprocessing/scannetpp_pth_preprocessing.py`

**处理流程** (`process_file` 函数, 80-208行):

1. 读取 `.pth` 文件，提取所有 `vtx_*` 数据
2. **创建特征矩阵** (180行):
   ```python
   points = np.hstack((
       coords,                              # (N, 3) - xyz坐标
       colors * 255,                        # (N, 3) - RGB颜色 [0-255]
       normals,                             # (N, 3) - 法向量
       unique_segment_ids[..., None],       # (N, 1) - 重映射后的segment ID
       semantic_labels[..., None],          # (N, 1) - 语义标签
       instance_labels[..., None]           # (N, 1) - 实例标签 (vtx_instance_anno_id)
   ))
   ```
   - `unique_segment_ids`: 使用 `np.unique(segment_ids, return_inverse=True)[1]` 将segment ID重映射为0,1,2,...

3. **创建GT文件** (183行):
   ```python
   gt_data = points[:, -2] * 1000 + points[:, -1] + 1
   ```
   - 格式: `semantic_id * 1000 + instance_id + 1`
   - 例如: semantic=15, instance=3 → 15004

4. 保存 `.npy` 文件 (12维特征矩阵)
5. 保存GT文件 (`.txt`，每行一个整数)
6. 创建 `label_database.yaml` 和 `train_database.yaml`

**输出文件**:
- `{scene_id}.npy`: (N, 12) 特征矩阵
- `instance_gt/{mode}/{scene_id}.txt`: GT标签（每行一个整数）
- `label_database.yaml`: 标签数据库
- `train_database.yaml`: 训练数据库

### 1.5 阶段4: 训练时数据加载

**数据集类**: `SemanticSegmentationDataset` (`datasets/semseg.py`)

**`__getitem__` 方法** (637-900行):

1. 加载 `.npy` 文件
2. 解析特征:
   ```python
   coordinates = points[:, :3]      # xyz
   color = points[:, 3:6]           # RGB [0-255]
   normals = points[:, 6:9]         # 法向量
   segments = points[:, 9]          # segment ID
   labels = points[:, 10:12]        # [semantic_label, instance_label]
   ```

3. 数据增强（训练时）:
   - 坐标归一化
   - 随机平移
   - 弹性形变
   - 颜色增强
   - 点云裁剪

4. 返回:
   ```python
   return (
       coordinates,      # (N, 3)
       features,         # (N, 6) - color + normals
       labels,           # (N, 2) - [sem, inst]
       scene_id,
       raw_color,
       raw_normals,
       raw_coordinates,
       idx,
       cap_data          # caption数据（如果有）
   )
   ```

**Collate函数**: `VoxelizeCollate` (`datasets/utils.py:10-63`)

**`voxelize` 函数** (`datasets/utils.py:231-429`):

1. **体素化** (265行):
   ```python
   coords = np.floor(sample[0] / voxel_size)  # 体素坐标
   _, _, unique_map, inverse_map = ME.utils.sparse_quantize(...)
   ```
   - 将点云转换为稀疏体素表示
   - `unique_map`: 去重后的体素索引
   - `inverse_map`: 每个点对应的体素索引

2. **创建SparseTensor** (使用MinkowskiEngine)

3. **处理标签** (根据task):
   - 对于instance_segmentation: 调用 `get_instance_masks` 创建target

4. **处理caption** (如果有)

**输出**:
- `coordinates`: SparseTensor坐标
- `features`: SparseTensor特征
- `labels`: 标签列表
- `target`: 实例分割target（包含masks和labels）

---

## 2. Label映射详解

### 2.1 Label映射的核心逻辑

Label映射分为两个层次：
1. **语义标签映射** (Semantic Label Mapping)
2. **实例标签映射** (Instance Label Mapping)

### 2.2 语义标签映射流程

#### 阶段1: prepare_training_data.py

**文件**: `semantic/transforms/mesh.py:10-96` (`MapLabelToIndex`)

**映射步骤**:

1. **读取类别列表**:
   - `top100.txt`: 100个标准语义类别
   - `map_benchmark.csv`: 原始标签 → 标准标签的映射表

2. **创建映射字典**:
   ```python
   # 第一层: 原始标签 → 标准标签
   label_mapping = {
       'books': 'book',
       'armchair': 'chair',
       ...
   }
   
   # 第二层: 标准标签 → 索引
   mapping = {
       'wall': 0,
       'floor': 1,
       'book': 15,
       ...
   }
   ```

3. **应用映射** (75-96行):
   ```python
   for anno in sample['anno']['segGroups']:
       label = anno['label']  # 原始标签，如 'books'
       
       # 步骤1: 原始标签 → 标准标签
       if label_mapping is not None:
           label = label_mapping.get(label, None)  # 'books' → 'book'
       
       # 步骤2: 标准标签 → 索引
       label_ndx = mapping.get(label, ignore_label)  # 'book' → 15
       
       anno['label_ndx'] = label_ndx
   ```

**结果**: 每个实例的 `label_ndx` 是 0-99 的整数（或-100表示忽略）

#### 阶段2: GetLabelsOnVertices

**文件**: `semantic/transforms/mesh.py:122-244`

**目的**: 将实例级别的语义标签映射到每个顶点

**关键代码** (179-202行):
```python
for instance_ndx, instance in enumerate(sample['anno']['segGroups']):
    if instance['label_ndx'] == self.ignore_label:
        continue
    
    # 找到属于该实例的所有顶点
    inst_mask = np.isin(seg_indices, instance['segments'])
    
    # 分配语义标签
    multilabels[inst_mask, new_label_position] = instance['label_ndx']
    
    # 如果是实例类别，分配实例标签
    if self.use_instances and instance['label'] in self.instance_labels:
        instance_multilabels[inst_mask, new_label_position] = instance_ndx
        instance_anno_id_multi[inst_mask, new_label_position] = instance['objectId']
```

**输出**:
- `vtx_labels`: 每个顶点的语义标签索引 (0-99)
- `vtx_instance_labels`: 每个顶点的实例标签索引 (0-N，仅对实例类别)
- `vtx_instance_anno_id`: 原始objectId

#### 阶段3: 训练时的标签映射

**文件**: `datasets/utils.py:212-228` (`get_sem_inst_mappings`)

**目的**: 创建语义类别到实例类别的映射（过滤掉非实例类别）

**逻辑**:
```python
def get_sem_inst_mappings(num_sem_classes, ignore_classes):
    # num_sem_classes: 100 (top100.txt的数量)
    # ignore_classes: 非实例类别的索引（如wall, floor）
    
    semid_to_instsemid = {}  # 0-99 → 0-84
    instsemid_to_semid = {}  # 0-84 → 0-99
    
    for semid in range(num_sem_classes):
        if semid in ignore_classes:
            continue  # 跳过非实例类别
        # 新ID = 已映射的类别数量
        semid_to_instsemid[semid] = len(semid_to_instsemid)
        instsemid_to_semid[len(instsemid_to_semid)] = semid
```

**示例**:
- top100中: wall(0), floor(1), chair(2), table(3), ...
- top100_instance中: chair, table, ... (没有wall, floor)
- 映射结果:
  - chair (semid=2) → instsemid=0
  - table (semid=3) → instsemid=1
  - ...

**应用位置**: `datasets/utils.py:589-597` (`get_instance_masks`函数)
```python
if num_classes is not None:
    l = torch.clone(label_ids)
    # 将语义ID映射到实例语义ID
    for label in semid_to_instsemid:
        l[label_ids == label] = semid_to_instsemid[label]
```

### 2.3 实例标签映射流程

#### 阶段1: prepare_training_data.py

**在GetLabelsOnVertices中** (195-198行):
```python
if self.use_instances and instance['label'] in self.instance_labels:
    # instance_ndx: 0, 1, 2, ... (按segGroups顺序)
    instance_multilabels[inst_mask, new_label_position] = instance_ndx
    # objectId: 原始标注中的ID（可能不连续）
    instance_anno_id_multi[inst_mask, new_label_position] = instance['objectId']
```

**注意**: 
- `instance_ndx`: 连续的索引 (0, 1, 2, ...)
- `objectId`: 原始标注中的ID（可能不连续，如 46, 47, 48, ...）

#### 阶段2: scannetpp_pth_preprocessing.py

**保存的实例标签** (139行):
```python
instance_labels = get_key('vtx_instance_anno_id', 'sampled_instance_anno_id')
```
- 这里保存的是 `objectId`（原始标注ID），不是 `instance_ndx`

#### 阶段3: 训练时

**在get_instance_masks中** (453-613行):

1. **提取实例** (476-499行):
   ```python
   for instance_id in instance_ids:
       if instance_id == -1 or instance_id < 0:
           continue
       
       # 获取该实例的语义标签
       tmp = list_labels[batch_id][list_labels[batch_id][:, 1] == instance_id]
       label_id = tmp[0, 0]  # 语义标签
       
       # 过滤非实例类别
       if label_id in filter_out_classes:
           continue
       
       label_ids.append(label_id)
       inst_ids.append(instance_id)  # 这是objectId
       masks.append(list_labels[batch_id][:, 1] == instance_id)
   ```

2. **语义标签映射** (589-597行):
   ```python
   if num_classes is not None:
       l = torch.clone(label_ids)
       # 将语义ID映射到实例语义ID
       for label in semid_to_instsemid:
           l[label_ids == label] = semid_to_instsemid[label]
   ```

**最终target**:
```python
target = {
    'labels': l,           # 实例语义ID (0-84)
    'masks': masks,        # 每个实例的mask
    'inst_ids': inst_ids   # 原始objectId
}
```

### 2.4 Label映射总结

**语义标签映射路径**:
```
原始标签 ('books') 
  → 标准标签 ('book') [MapLabelToIndex]
  → 语义索引 (15) [MapLabelToIndex]
  → 顶点标签 (15) [GetLabelsOnVertices]
  → 实例语义索引 (10) [get_sem_inst_mappings, 如果chair在top100_instance中是第10个]
```

**实例标签映射路径**:
```
objectId (46) [segments_anno.json]
  → instance_ndx (0) [GetLabelsOnVertices, 按segGroups顺序]
  → objectId (46) [保存到pth文件]
  → instance_id (46) [训练时使用]
```

**关键点**:
1. 语义标签从100类映射到84类（过滤非实例类别）
2. 实例标签使用原始objectId，不进行重映射
3. 非实例类别（wall, floor等）的实例标签为-100

---

## 3. 模型架构详解

### 3.1 整体架构

ExCap3D包含两个主要组件：
1. **Mask3D**: 实例分割模型
2. **Mask3D_Captioner**: 描述生成模型

### 3.2 Mask3D模型架构

**文件**: `models/mask3d.py`

#### 3.2.1 输入

**数据类型**: MinkowskiEngine SparseTensor
- `coordinates`: 体素坐标 (整数)
- `features`: 体素特征 (N, 6) - [RGB(3) + normals(3)]

#### 3.2.2 Backbone: Res16UNetBase

**文件**: `models/res16unet.py`

**功能**: 提取多尺度特征

**输出**:
- `pcd_features`: 最细粒度的特征 (N, 96)
- `aux`: 多尺度特征列表
  ```python
  aux = [
      (n1, 256),  # 最粗尺度
      (n2, 256),
      (n3, 128),
      (n4, 96),
      (N, 96),    # 最细尺度
  ]
  ```

#### 3.2.3 Mask Features Head

**代码位置**: `mask3d.py:283`
```python
mask_features = self.mask_features_head(pcd_features)  # N, 128
```

**功能**: 将backbone特征转换为mask特征

#### 3.2.4 Query生成

**代码位置**: `mask3d.py:310-361`

**方法**: Non-parametric queries (默认)

1. **FPS采样** (313-320行):
   ```python
   fps_idx = [
       furthest_point_sample(
           x.decomposed_coordinates[i][None, ...].float(),
           self.num_queries,  # 默认100
       )
       for i in range(batch_size)
   ]
   ```
   - 在每个场景的点云上使用FPS采样100个查询点

2. **获取查询坐标** (323-328行):
   ```python
   sampled_coords = torch.stack([
       coordinates.decomposed_features[i][fps_idx[i].long(), :]
       for i in range(len(fps_idx))
   ])  # B, 100, 3
   ```

3. **位置编码** (345-348行):
   ```python
   query_pos = self.pos_enc(sampled_coords.float(), input_range=[mins, maxs])
   query_pos = self.query_projection(query_pos)  # B, 128, 100
   ```

4. **查询特征** (350-361行):
   ```python
   if not self.use_np_features:
       queries = torch.zeros_like(query_pos).permute((0, 2, 1))  # B, 100, 128
   else:
       queries = torch.stack([
           pcd_features.decomposed_features[i][fps_idx[i].long(), :]
           for i in range(len(fps_idx))
       ])
   ```

**输出**:
- `queries`: (B, 100, 128) - 查询特征
- `query_pos`: (100, B, 128) - 查询位置编码

#### 3.2.5 Decoder

**代码位置**: `mask3d.py:414-622`

**结构**: 多个decoder层（默认3层），每层包含：

1. **Mask Module** (422-441行):
   ```python
   output_class, outputs_mask, attn_mask = self.mask_module(
       queries,
       mask_features,
       mask_segments,  # 如果train_on_segments=True
       num_pooling_steps,
       ret_attn_mask=True,
       point2segment=point2segment,
   )
   ```
   
   **功能**:
   - `output_class`: (B, 100, num_classes+1) - 类别预测
   - `outputs_mask`: 每个查询的mask预测
   - `attn_mask`: 注意力mask

2. **Cross-Attention** (444-527行):
   ```python
   # 在不同尺度特征上进行cross-attention
   for hlevel in self.hlevels:  # [0, 1, 2, 3]
       decomposed_aux = aux[hlevel].decomposed_features
       # Cross-attention: queries ← features
       queries = self.cross_attention(
           queries,
           decomposed_aux,
           attn_mask,
       )
   ```

3. **Self-Attention** (528-540行):
   ```python
   queries = self.self_attention(queries)
   ```

#### 3.2.6 Mask Module详解

**代码位置**: `mask3d.py:645-704`

**功能**: 从查询特征生成类别和mask预测

**步骤**:

1. **类别预测** (657行):
   ```python
   outputs_class = self.class_embed_head(query_feat)  # (B, 100, num_classes+1)
   ```

2. **Mask嵌入** (656行):
   ```python
   mask_embed = self.mask_embed_head(query_feat)  # (B, 100, 128)
   ```

3. **Mask预测** (661-670行):
   ```python
   if point2segment is not None:
       # 在segment级别预测
       output_segments = mask_segments[i] @ mask_embed[i].T  # (n_segments, 100)
       output_masks = output_segments[point2segment[i]]  # (n_voxels, 100)
   else:
       # 在voxel级别预测
       output_masks = mask_features.decomposed_features[i] @ mask_embed[i].T
   ```

**关键点**:
- 如果 `train_on_segments=True`，mask在segment级别预测，然后通过 `point2segment` 映射到voxel
- 否则直接在voxel级别预测

#### 3.2.7 输出

**代码位置**: `mask3d.py:629-643`

```python
return {
    "pred_logits": predictions_class[-1],      # (B, 100, num_classes+1)
    "pred_masks": predictions_mask[-1],         # List of (n_voxels, 100)
    "aux_outputs": self._set_aux_loss(...),     # 中间层输出
    "sampled_coords": sampled_coords,           # (B, 100, 3)
    "backbone_features": pcd_features,
    "final_queries": queries,                   # (B, 100, 128)
    "attn_masks": pred_attn_masks[-1],
}
```

### 3.3 Mask3D_Captioner模型架构

**文件**: `models/mask3d_captioner/mask3d_captioner.py`

#### 3.3.1 输入

- `instseg_output`: Mask3D的输出
- `assignment`: Hungarian匹配结果
- `caption_gt`: Caption ground truth

#### 3.3.2 查询特征提取

**代码位置**: `mask3d_captioner.py:604-700`

1. **获取匹配的查询** (610-620行):
   ```python
   # 从assignment中获取匹配的查询索引
   matched_query_indices = assignment[0][0]  # 匹配的查询索引
   matched_queries = instseg_output['final_queries'][0][matched_query_indices]
   ```

2. **语义one-hot编码** (625-635行):
   ```python
   if self.gt_sem_onehot_as_query:
       # 使用GT语义标签
       sem_onehot = F.one_hot(gt_sem_ids, num_classes=self.num_classes)
   elif self.pred_sem_onehot_as_query:
       # 使用预测的语义标签
       pred_sem_ids = torch.argmax(instseg_output['pred_logits'], dim=-1)
       sem_onehot = F.one_hot(pred_sem_ids, num_classes=self.num_classes)
   ```

3. **查询特征组合** (640-650行):
   ```python
   if self.dont_project_queries:
       query_feats = matched_queries
   else:
       query_feats = self.query_projection(matched_queries)
   
   # 组合查询特征和语义one-hot
   input_feats = torch.cat([query_feats, sem_onehot], dim=-1)
   ```

#### 3.3.3 GPT模型

**代码位置**: `mask3d_captioner.py:650-700`

1. **Tokenization** (655-665行):
   ```python
   caption_ids = self.tokenizer(
       captions,
       padding=True,
       return_tensors='pt',
   )['input_ids']
   ```

2. **GPT Forward** (670-680行):
   ```python
   caption_output = self.gpt_model(
       input_ids=caption_ids,
       inputs_embeds=input_feats,  # 使用查询特征作为输入
   )
   ```

3. **Loss计算** (754-761行):
   ```python
   caption_loss = self.loss_caption(
       logits=caption_output.logits[:, (num_prefix_tokens-1):-1, :],
       target=caption_ids_picked.long(),
       weights=weights,  # 类别权重
   )
   ```

#### 3.3.4 Loss函数

**代码位置**: `mask3d_captioner.py:776-795`

```python
def loss_caption(self, logits, target, weights=None):
    loss_per_word = nnf.cross_entropy(
        logits.reshape(-1, self.nvocabs),
        target.reshape(-1),
        reduction='none',
        ignore_index=0,  # padding
    )
    
    if weights is not None:
        loss_per_word = loss_per_word * weights.unsqueeze(1)
    
    final_loss = torch.sum(loss_per_word * (target != 0).float()) / \
                 torch.sum((target != 0).float() + 1e-6)
    return final_loss
```

### 3.4 模型预测流程总结

1. **点云输入** → 体素化 → SparseTensor
2. **Backbone** → 多尺度特征
3. **FPS采样** → 100个查询点
4. **Decoder** → 类别和mask预测
5. **Hungarian匹配** → 匹配预测和GT
6. **Captioner** → 为匹配的实例生成描述

**关键点**:
- 预测是在voxel/segment级别进行的，不是每个点
- 100个查询对应最多100个预测实例
- 通过Hungarian算法匹配预测和GT

---

## 4. Loss和评估参数详解

### 4.1 Loss函数

#### 4.1.1 SetCriterion

**文件**: `models/criterion.py:93-349`

**功能**: 计算实例分割的loss

**步骤**:

1. **Hungarian匹配** (295行):
   ```python
   indices = self.matcher(outputs_without_aux, targets, mask_type)
   ```
   - 匹配预测和GT实例
   - `indices[i] = (src_idx, tgt_idx)` - 预测索引和GT索引的对应

2. **Loss计算** (309-315行):
   ```python
   for loss in self.losses:  # ['labels', 'masks']
       losses.update(
           self.get_loss(loss, outputs, targets, indices, num_masks, mask_type)
       )
   ```

#### 4.1.2 Classification Loss (loss_labels)

**代码位置**: `models/criterion.py:143-169`

```python
def loss_labels(self, outputs, targets, indices, num_masks, mask_type):
    src_logits = outputs["pred_logits"].float()  # (B, 100, num_classes+1)
    
    # 获取匹配的GT类别
    idx = self._get_src_permutation_idx(indices)
    target_classes_o = torch.cat([t["labels"][J] for t, (_, J) in zip(targets, indices)])
    
    # 创建target tensor，未匹配的为num_classes (no-object)
    target_classes = torch.full(
        src_logits.shape[:2],
        self.num_classes,  # no-object类别
        dtype=torch.int64,
        device=src_logits.device,
    )
    target_classes[idx] = target_classes_o
    
    # Cross-entropy loss
    loss_ce = F.cross_entropy(
        src_logits.transpose(1, 2),  # (B, num_classes+1, 100)
        target_classes,              # (B, 100)
        self.empty_weight,          # 类别权重
        ignore_index=253,
    )
    return {"loss_ce": loss_ce}
```

**关键点**:
- 使用类别权重 (`empty_weight`) 平衡不同类别的loss
- 未匹配的查询被标记为 `num_classes` (no-object)

#### 4.1.3 Mask Loss (loss_masks)

**代码位置**: `models/criterion.py:171-208`

```python
def loss_masks(self, outputs, targets, indices, num_masks, mask_type):
    loss_masks = []
    loss_dices = []
    
    for batch_id, (map_id, target_id) in enumerate(indices):
        # 获取匹配的预测mask和GT mask
        map = outputs["pred_masks"][batch_id][:, map_id].T  # (n_masks, n_voxels)
        target_mask = targets[batch_id][mask_type][target_id]  # (n_masks, n_voxels)
        
        # 随机采样点（如果num_points != -1）
        if self.num_points != -1:
            point_idx = torch.randperm(target_mask.shape[1])[:int(self.num_points * target_mask.shape[1])]
        else:
            point_idx = torch.arange(target_mask.shape[1])
        
        map = map[:, point_idx]
        target_mask = target_mask[:, point_idx].float()
        
        # Binary cross-entropy loss
        loss_masks.append(sigmoid_ce_loss_jit(map, target_mask, num_masks))
        # Dice loss
        loss_dices.append(dice_loss_jit(map, target_mask, num_masks))
    
    return {
        "loss_mask": torch.sum(torch.stack(loss_masks)),
        "loss_dice": torch.sum(torch.stack(loss_dices)),
    }
```

**两种Loss**:

1. **Sigmoid CE Loss** (`sigmoid_ce_loss`, 49-68行):
   ```python
   loss = F.binary_cross_entropy_with_logits(
       inputs, targets, reduction="none"
   )
   return loss.mean(1).sum() / num_masks
   ```

2. **Dice Loss** (`dice_loss`, 24-43行):
   ```python
   inputs = inputs.sigmoid()
   inputs = inputs.flatten(1)
   numerator = 2 * (inputs * targets).sum(-1)
   denominator = inputs.sum(-1) + targets.sum(-1)
   loss = 1 - (numerator + 1) / (denominator + 1)
   return loss.sum() / num_masks
   ```

#### 4.1.4 Caption Loss

**代码位置**: `models/mask3d_captioner/mask3d_captioner.py:776-795`

```python
def loss_caption(self, logits, target, weights=None):
    # logits: (nobj, ntoken, vocab_size)
    # target: (nobj, ntoken)
    
    loss_per_word = nnf.cross_entropy(
        logits.reshape(-1, self.nvocabs),
        target.reshape(-1),
        reduction='none',
        ignore_index=0,  # padding token
    )
    
    # 重塑为 (nobj, ntoken)
    loss_per_word = loss_per_word.reshape(target.shape)
    
    # 应用类别权重
    if weights is not None:
        loss_per_word = loss_per_word * weights.unsqueeze(1)
    
    # 只计算非padding token的loss
    final_loss = torch.sum(loss_per_word * (target != 0).float()) / \
                 torch.sum((target != 0).float() + 1e-6)
    return final_loss
```

#### 4.1.5 总Loss

**代码位置**: `trainer/trainer.py:439-440`

```python
total_loss = sum(losses.values())
```

**Loss组成**:
- `loss_ce`: 分类loss
- `loss_mask`: Mask的BCE loss
- `loss_dice`: Mask的Dice loss
- `loss_caption`: Caption的交叉熵loss（如果有）

### 4.2 评估参数

#### 4.2.1 AP (Average Precision)

**文件**: `utils/votenet_utils/eval_det.py`

**计算流程**:

1. **IoU计算** (`get_iou`, 63-95行):
   ```python
   def get_iou(pred_mask, gt_mask):
       intersection = (pred_mask & gt_mask).sum()
       union = (pred_mask | gt_mask).sum()
       return intersection / (union + 1e-6)
   ```

2. **匹配预测和GT** (`eval_det_cls`, 100-198行):
   - 对每个预测，找到IoU最大的GT
   - 如果IoU > threshold，匹配成功

3. **计算Precision和Recall** (199-260行):
   ```python
   # 按置信度排序预测
   sorted_indices = np.argsort(-scores)
   
   tp = 0  # true positive
   fp = 0  # false positive
   
   for idx in sorted_indices:
       if matched[idx]:
           tp += 1
       else:
           fp += 1
       
       precision[idx] = tp / (tp + fp)
       recall[idx] = tp / num_gt
   ```

4. **计算AP** (`voc_ap`, 25-56行):
   ```python
   def voc_ap(rec, prec, use_07_metric=False):
       if use_07_metric:
           # 11点方法
           ap = 0.0
           for t in np.arange(0.0, 1.1, 0.1):
               if np.sum(rec >= t) == 0:
                   p = 0
               else:
                   p = np.max(prec[rec >= t])
               ap = ap + p / 11.0
       else:
           # 精确方法（默认）
           mrec = np.concatenate(([0.0], rec, [1.0]))
           mpre = np.concatenate(([0.0], prec, [0.0]))
           
           # 计算precision envelope
           for i in range(mpre.size - 1, 0, -1):
               mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])
           
           # 计算AP
           i = np.where(mrec[1:] != mrec[:-1])[0]
           ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
       return ap
   ```

**评估指标**:
- `AP@0.25`: IoU threshold = 0.25
- `AP@0.5`: IoU threshold = 0.5
- `mAP`: 所有类别的平均AP

#### 4.2.2 评估流程

**文件**: `trainer/trainer.py:1807-2585`

**`eval_instance_step` 函数** (1807-2350行):

1. **获取预测** (1850-1900行):
   ```python
   pred_logits = output["pred_logits"]  # (B, 100, num_classes+1)
   pred_masks = output["pred_masks"]    # List of (n_voxels, 100)
   
   # 应用sigmoid和阈值
   pred_scores = pred_logits.softmax(dim=-1)[:, :, :-1]  # 排除no-object
   pred_classes = pred_scores.argmax(dim=-1)
   ```

2. **匹配预测和GT** (1900-1950行):
   ```python
   # 使用Hungarian算法匹配
   matches = hungarian_matching(pred_masks, target_full)
   ```

3. **计算IoU** (1950-2000行):
   ```python
   for pred_idx, gt_idx in matches:
       iou = compute_iou(pred_masks[pred_idx], target_full[gt_idx]['masks'])
   ```

4. **计算AP** (2000-2100行):
   ```python
   ap_results = compute_ap(pred_scores, matches, ious, iou_thresholds=[0.25, 0.5])
   ```

**`eval_instance_epoch_end` 函数** (2352-2585行):

1. **聚合所有场景的结果** (2400-2500行)

2. **计算平均AP** (2550-2574行):
   ```python
   mean_ap = np.nanmean([item for key, item in ap_results.items() if key.endswith("val_ap")])
   mean_ap_50 = np.nanmean([item for key, item in ap_results.items() if key.endswith("val_ap_50")])
   mean_ap_25 = np.nanmean([item for key, item in ap_results.items() if key.endswith("val_ap_25")])
   ```

#### 4.2.3 Caption评估

**文件**: `eval_caps.py`

**指标**: CIDEr, BLEU, METEOR, ROUGE

**计算流程**:
1. 生成caption预测
2. 与GT caption比较
3. 计算各种指标

### 4.3 评估参数总结

**实例分割指标**:
- `AP@0.25`: IoU threshold = 0.25的平均精度
- `AP@0.5`: IoU threshold = 0.5的平均精度
- `mAP`: 所有类别的平均AP
- `mAP@0.25`, `mAP@0.5`: 平均mAP

**Caption指标**:
- `CIDEr`: Caption相似度
- `BLEU`: N-gram匹配
- `METEOR`: 基于同义词的匹配
- `ROUGE`: 召回导向的匹配

**Loss权重**:
- `loss_ce`: 分类loss权重
- `loss_mask`: Mask BCE loss权重
- `loss_dice`: Mask Dice loss权重
- `loss_caption`: Caption loss权重（如果有）

---

## 5. 关键代码位置总结

### 5.1 数据流程
- `scannetpp/semantic/prep/prepare_training_data.py`: 初始数据处理
- `scannetpp/semantic/transforms/mesh.py`: 标签映射和采样
- `ExCap3D/sample_pth.py`: 重新采样
- `ExCap3D/datasets/preprocessing/scannetpp_pth_preprocessing.py`: 预处理
- `ExCap3D/datasets/semseg.py`: 数据集类
- `ExCap3D/datasets/utils.py`: VoxelizeCollate

### 5.2 Label映射
- `scannetpp/semantic/transforms/mesh.py:10-96`: MapLabelToIndex
- `scannetpp/semantic/transforms/mesh.py:122-244`: GetLabelsOnVertices
- `ExCap3D/datasets/utils.py:212-228`: get_sem_inst_mappings
- `ExCap3D/datasets/utils.py:453-613`: get_instance_masks

### 5.3 模型架构
- `ExCap3D/models/mask3d.py`: Mask3D主模型
- `ExCap3D/models/res16unet.py`: Backbone
- `ExCap3D/models/mask3d_captioner/mask3d_captioner.py`: Caption模型

### 5.4 Loss和评估
- `ExCap3D/models/criterion.py`: SetCriterion
- `ExCap3D/models/mask3d_captioner/mask3d_captioner.py:776-795`: Caption loss
- `ExCap3D/trainer/trainer.py:1807-2585`: 评估函数
- `ExCap3D/utils/votenet_utils/eval_det.py`: AP计算

---

## 6. 常见问题解答

### Q1: 为什么预测的instance数量有时候很多？
**A**: 模型固定预测100个实例（`num_queries=100`）。通过Hungarian匹配，只有匹配到GT的预测才被认为是有效实例。未匹配的预测被标记为no-object。

### Q2: 模型是在每个voxel/point上做预测吗？
**A**: 不是。模型在segment级别或voxel级别预测mask，但预测的是100个查询对应的mask，不是每个点一个预测。每个查询对应一个实例。

### Q3: semantic_label和instance_label的区别？
**A**: 
- `semantic_label`: 语义类别（如chair, table），范围0-99（top100）或0-84（top100_instance）
- `instance_label`: 实例ID（区分同一类别的不同实例），使用原始objectId

### Q4: label映射会出错吗？
**A**: 映射逻辑是正确的，但需要注意：
1. `top100.txt`和`top100_instance.txt`必须一致
2. `map_benchmark.csv`的映射关系必须正确
3. 非实例类别的instance_label应该是-100

---

## 7. 核心逻辑简化总结

### 7.1 Label映射核心逻辑

```
原始标签 → 标准标签 → 语义索引(0-99) → 实例语义索引(0-84)
         [MapLabelToIndex]  [GetLabelsOnVertices]  [get_sem_inst_mappings]
```

### 7.2 模型预测核心逻辑

```
点云 → 体素化 → Backbone → 100个查询 → Decoder → 类别+Mask → 匹配GT → Loss
```

### 7.3 评估核心逻辑

```
预测Mask + GT Mask → IoU计算 → 匹配 → Precision/Recall → AP
```

---

本文档详细解释了ExCap3D的完整数据流程、label映射、模型架构和评估方法。如有疑问，请参考相应的代码文件。

